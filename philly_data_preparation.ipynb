{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Philly data preparation\n",
    "\n",
    "Weifan jiang, weifanjiang@g.harvard.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse string date\n",
    "def parse_date(date_str):\n",
    "    if date_str is None or date_str == '' or date_str == 'None':\n",
    "        return None\n",
    "    if date_str.endswith(\"PST\") or date_str.endswith(\"PDT\"):\n",
    "        date_str = date_str[:-4]\n",
    "    return datetime.datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "# convert time delta object to number of minutes\n",
    "def timedelta_to_minutes(timedelta):\n",
    "    minutes = 0.0\n",
    "    minutes += timedelta.days * 24 * 60\n",
    "    minutes += timedelta.seconds / 60.0\n",
    "    minutes += timedelta.microseconds / (60 * 1000)\n",
    "    return minutes\n",
    "\n",
    "\n",
    "# count the number of machines that a job is scheduled on\n",
    "# returns (cpu count, gpu count)\n",
    "def count_machines(detail):\n",
    "    cpu_count = len(detail)\n",
    "    gpu_count = 0\n",
    "    for machine in detail:\n",
    "        gpu_count += len(machine[\"gpus\"])\n",
    "    return cpu_count, gpu_count\n",
    "\n",
    "\n",
    "# read csv (the format is not compatible with pandas.read_csv)\n",
    "def philly_read_csv(fpath, max_lines):\n",
    "    columns = None\n",
    "    data_lists = None\n",
    "    with open(fpath, \"r\") as fin:\n",
    "        reader = csv.reader(fin)\n",
    "        columns = [x.strip() for x in next(reader)]\n",
    "        data_lists = [list() for _ in columns]\n",
    "        pbar = tqdm.tqdm_notebook(total=max_lines)\n",
    "        for row_raw in reader:\n",
    "            pbar.update(1)\n",
    "            row = row_raw\n",
    "\n",
    "            # special case for file misformat in gpu utilization trace\n",
    "            if len(row) != len(columns):\n",
    "                if row[-1] == \"\":\n",
    "                    row = row[:-1]\n",
    "                if len(row) == 18 and len(columns) == 10:\n",
    "                    row = row[0:2] + row[2:][::2]\n",
    "                row = row + [None, ] * (len(columns) - len(row))\n",
    "            \n",
    "            # remove the time zone\n",
    "            if columns[0] == 'time':\n",
    "                row[0] = row[0][:-4]\n",
    "            \n",
    "            for idx, element in enumerate(row):\n",
    "                data_lists[idx].append(element)\n",
    "        pbar.close()\n",
    "    data_dict = dict()\n",
    "    for colname, elements in zip(columns, data_lists):\n",
    "        data_dict[colname] = elements\n",
    "    return pd.DataFrame(data=data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "np.random.seed(10)\n",
    "random.seed(10)\n",
    "\n",
    "\n",
    "# data location\n",
    "trace_dir = \"philly-traces/trace-data/\"\n",
    "job_log_path = os.path.join(trace_dir, \"cluster_job_log\")\n",
    "output_dir = \"data/philly\"\n",
    "sampled_jobs_path = os.path.join(output_dir, \"sampled_jobs.json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "job_data_dir = os.path.join(output_dir, \"job_data\")\n",
    "os.makedirs(job_data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(sampled_jobs_path):\n",
    "    # read full data\n",
    "    with open(job_log_path, \"r\") as fin:\n",
    "        job_log = json.load(fin)\n",
    "    \n",
    "\n",
    "    # filter for jobs with one attempt\n",
    "    jobs_single_attempt = [x for x in job_log if len(x[\"attempts\"]) == 1]\n",
    "    # jobs with complete runtime properties\n",
    "    for job in jobs_single_attempt:\n",
    "        start_time = parse_date(job[\"attempts\"][0][\"start_time\"])\n",
    "        end_time = parse_date(job[\"attempts\"][0][\"end_time\"])\n",
    "        if start_time is not None and end_time is not None:\n",
    "            job[\"runtime_min\"] = timedelta_to_minutes(end_time - start_time)\n",
    "        else:\n",
    "            job[\"runtime_min\"] = None\n",
    "    jobs_single_attempt = [x for x in jobs_single_attempt if x['runtime_min'] is not None]\n",
    "    # filter for jobs that lasted for at list 5 minutes\n",
    "    jobs_single_attempt = [x for x in jobs_single_attempt if 5 <= x['runtime_min']]\n",
    "    # try to select jobs scheduled on multiple GPUs\n",
    "    jobs_single_attempt = [\n",
    "        x for x in jobs_single_attempt if count_machines(x[\"attempts\"][0][\"detail\"])[1] > 1\n",
    "    ]\n",
    "\n",
    "\n",
    "    # check the distribution of jobs with different final status\n",
    "    jobs_pass = [x for x in jobs_single_attempt if x[\"status\"] == \"Pass\"]\n",
    "    jobs_killed = [x for x in jobs_single_attempt if x[\"status\"] == \"Killed\"]\n",
    "    jobs_failed = [x for x in jobs_single_attempt if x[\"status\"] == \"Failed\"]\n",
    "    print('job status: Pass ({}), Killed ({}), Failed ({})'.format(\n",
    "        len(jobs_pass), len(jobs_killed), len(jobs_failed)\n",
    "    ))\n",
    "\n",
    "\n",
    "    # sample size: min length of qualified jobs in each status\n",
    "    output_json = list()\n",
    "    job_size = np.amin([len(jobs_pass), len(jobs_killed), len(jobs_failed)])\n",
    "    pbar = tqdm.tqdm_notebook(total=job_size * 3, desc=\"extract sampled jobs\")\n",
    "    for jobs in [jobs_pass, jobs_killed, jobs_failed]:\n",
    "        sampled_jobs = random.sample(jobs, job_size)\n",
    "        for job in sampled_jobs:\n",
    "            pbar.update(1)\n",
    "            output_job = dict()\n",
    "            for key in (\"status\", \"vc\", \"jobid\", \"submitted_time\", \"user\", \"runtime_min\"):\n",
    "                output_job[key] = job[key]\n",
    "            for key in (\"start_time\", \"end_time\", \"detail\"):\n",
    "                output_job[key] = job[\"attempts\"][0][key]\n",
    "            output_json.append(output_job)\n",
    "    pbar.close()\n",
    "    with open(sampled_jobs_path, \"w\") as fout:\n",
    "        json.dump(output_json, fout, indent=2)\n",
    "\n",
    "\n",
    "with open(sampled_jobs_path, \"r\") as fin:\n",
    "    sampled_jobs = json.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load full cluster traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_df = philly_read_csv(os.path.join(trace_dir, \"cluster_gpu_util\", 44750641))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e26a65d6bcde696dc952d5a33c2326785fe062ace10d8c88a4674d4f129a0e9b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('msr_philly')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
