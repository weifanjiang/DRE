{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Philly data preparation\n",
    "\n",
    "Weifan jiang, weifanjiang@g.harvard.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import csv\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse string date\n",
    "def parse_date(date_str):\n",
    "    if date_str is None or date_str == '' or date_str == 'None':\n",
    "        return None\n",
    "    if date_str.endswith(\"PST\") or date_str.endswith(\"PDT\"):\n",
    "        date_str = date_str[:-4]\n",
    "    return datetime.datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "# convert time delta object to number of minutes\n",
    "def timedelta_to_minutes(timedelta):\n",
    "    minutes = 0.0\n",
    "    minutes += timedelta.days * 24 * 60\n",
    "    minutes += timedelta.seconds / 60.0\n",
    "    minutes += timedelta.microseconds / (60 * 1000)\n",
    "    return minutes\n",
    "\n",
    "\n",
    "# convert datetime object to string representation\n",
    "def datetime_to_str(t):\n",
    "    return t.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "\n",
    "# adjust current time by several minutes\n",
    "def change_time_by_min(t, minutes):\n",
    "    return t + datetime.timedelta(seconds=60*minutes)\n",
    "\n",
    "\n",
    "# count the number of machines that a job is scheduled on\n",
    "# returns (cpu count, gpu count)\n",
    "def count_machines(detail):\n",
    "    cpu_count = len(detail)\n",
    "    gpu_count = 0\n",
    "    for machine in detail:\n",
    "        gpu_count += len(machine[\"gpus\"])\n",
    "    return cpu_count, gpu_count\n",
    "\n",
    "\n",
    "# rounds a datetime object down to the nearest minute\n",
    "def round_to_nearest_minute(t):\n",
    "    return t - datetime.timedelta(seconds=t.second, microseconds=t.microsecond)\n",
    "\n",
    "\n",
    "# get the time interval from lo to hi, centered at given time\n",
    "def get_time_interval(center, lo, hi):\n",
    "    dt = round_to_nearest_minute(parse_date(center))\n",
    "    return [datetime_to_str(change_time_by_min(dt, x)) for x in range(lo, hi + 1)]\n",
    "\n",
    "\n",
    "# read csv (the format is not compatible with pandas.read_csv)\n",
    "def philly_read_csv(fpath, max_lines, desc='loading'):\n",
    "    columns = None\n",
    "    data_lists = None\n",
    "    with open(fpath, \"r\") as fin:\n",
    "        reader = csv.reader(fin)\n",
    "        columns = [x.strip() for x in next(reader)]\n",
    "        data_lists = [list() for _ in columns]\n",
    "        pbar = tqdm(total=max_lines, desc=desc)\n",
    "        for row_raw in reader:\n",
    "            pbar.update(1)\n",
    "\n",
    "            # handle missing data: replace \"NA\" with None\n",
    "            row = [x if x != \"NA\" else -1 for x in row_raw]\n",
    "\n",
    "            # special case for file misformat in gpu utilization trace\n",
    "            if len(row) != len(columns):\n",
    "                if row[-1] == \"\":\n",
    "                    row = row[:-1]\n",
    "                if len(row) == 18 and len(columns) == 10:\n",
    "                    row = row[0:2] + row[2:][::2]\n",
    "                row = row + [None, ] * (len(columns) - len(row))\n",
    "            \n",
    "            # remove the time zone\n",
    "            if columns[0] == 'time':\n",
    "                row[0] = row[0][:-4]\n",
    "            \n",
    "            for idx, element in enumerate(row):\n",
    "                data_lists[idx].append(element)\n",
    "        pbar.close()\n",
    "    data_dict = dict()\n",
    "    for colname, elements in zip(columns, data_lists):\n",
    "        data_dict[colname] = elements\n",
    "    return pd.DataFrame(data=data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed\n",
    "np.random.seed(10)\n",
    "random.seed(10)\n",
    "\n",
    "\n",
    "# data location\n",
    "trace_dir = \"philly-traces/trace-data/\"\n",
    "job_log_path = os.path.join(trace_dir, \"cluster_job_log\")\n",
    "output_dir = \"data/philly\"\n",
    "sampled_jobs_path = os.path.join(output_dir, \"sampled_jobs.json\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "for name in ['gpu_util', 'cpu_util', 'mem_util']:\n",
    "    job_data_dir = os.path.join(output_dir, name)\n",
    "    os.makedirs(job_data_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# schema\n",
    "time_lo = -10\n",
    "time_hi = 3\n",
    "output_columns = [\"name\", \"machine_type\", \"trace\", ]\n",
    "for i in range(time_lo, time_hi + 1):\n",
    "    output_columns.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total job count 5910\n",
      "Pass jobs 3172, failed jobs 2738\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(sampled_jobs_path):\n",
    "\n",
    "    min_start_time = \"2017-10-03 01:00:00\"\n",
    "    max_start_time = \"2017-12-15 17:42:00\"\n",
    "\n",
    "    # read full data\n",
    "    with open(job_log_path, \"r\") as fin:\n",
    "        job_log = json.load(fin)\n",
    "    \n",
    "\n",
    "    # only keep the last attempt of jobs\n",
    "    jobs_single_attempt = list()\n",
    "    for job in job_log:\n",
    "        if len(job[\"attempts\"]) > 1:\n",
    "            job[\"attempts\"] = [job[\"attempts\"][-1], ]\n",
    "        # select for pass/fail jobs\n",
    "        if job['status'] in ('Pass', 'Failed') and len(job[\"attempts\"]) == 1:\n",
    "            jobs_single_attempt.append(job)\n",
    "\n",
    "    # jobs with complete runtime properties\n",
    "    for job in jobs_single_attempt:\n",
    "        start_time = parse_date(job[\"attempts\"][0][\"start_time\"])\n",
    "        end_time = parse_date(job[\"attempts\"][0][\"end_time\"])\n",
    "        if start_time is not None and end_time is not None:\n",
    "            job[\"runtime_min\"] = timedelta_to_minutes(end_time - start_time)\n",
    "        else:\n",
    "            job[\"runtime_min\"] = None\n",
    "    jobs_single_attempt = [x for x in jobs_single_attempt if x['runtime_min'] is not None]\n",
    "    # filter for jobs that lasted for at list 5 minutes\n",
    "    jobs_single_attempt = [x for x in jobs_single_attempt if 5 <= x['runtime_min']]\n",
    "    # try to select jobs scheduled on multiple GPUs\n",
    "    jobs_single_attempt = [\n",
    "        x for x in jobs_single_attempt if count_machines(x[\"attempts\"][0][\"detail\"])[1] > 1\n",
    "    ]\n",
    "\n",
    "    # format output\n",
    "    output_json = list()\n",
    "    pbar = tqdm(total=len(jobs_single_attempt), desc=\"extract sampled jobs\")\n",
    "    for job in jobs_single_attempt:\n",
    "        pbar.update(1)\n",
    "        output_job = dict()\n",
    "        for key in (\"status\", \"vc\", \"jobid\", \"submitted_time\", \"user\", \"runtime_min\"):\n",
    "            output_job[key] = job[key]\n",
    "        for key in (\"start_time\", \"end_time\", \"detail\"):\n",
    "            output_job[key] = job[\"attempts\"][0][key]\n",
    "        \n",
    "        # filter for time interval with cpu/gpu/mem logs\n",
    "        if output_job[\"start_time\"] >= min_start_time and output_job[\"start_time\"] <= max_start_time:\n",
    "            output_json.append(output_job)\n",
    "    pbar.close()\n",
    "    with open(sampled_jobs_path, \"w\") as fout:\n",
    "        json.dump(output_json, fout, indent=2)\n",
    "\n",
    "\n",
    "with open(sampled_jobs_path, \"r\") as fin:\n",
    "    sampled_jobs = json.load(fin)\n",
    "\n",
    "total_job_count = len(sampled_jobs)\n",
    "pass_job_count = len([x for x in sampled_jobs if x['status'] == 'Pass'])\n",
    "failed_job_count = len([x for x in sampled_jobs if x['status'] == 'Failed'])\n",
    "\n",
    "print('total job count {}'.format(total_job_count))\n",
    "print('Pass jobs {}, failed jobs {}'.format(pass_job_count, failed_job_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## per-job gpu trace collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf3b784122243eaaf5e5164c154512d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "load gpu utilization traces:   0%|          | 0/44750640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpu_df = philly_read_csv(os.path.join(trace_dir, \"cluster_gpu_util\"), 44750640, \"load gpu utilization traces\")\n",
    "gpu_df.set_index([\"time\", \"machineId\"], inplace=True)\n",
    "gpu_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231fe125835540b0a0a5947cb9c003a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "collect job gpu utilizations:   0%|          | 0/5910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for job in tqdm(sampled_jobs, desc='collect job gpu utilizations'):\n",
    "    job_trace_path = os.path.join(output_dir, \"gpu_util\", \"{}.csv\".format(job[\"jobid\"]))\n",
    "    if not os.path.isfile(job_trace_path):\n",
    "        data_lists = [list() for _ in output_columns]\n",
    "        selected_times = get_time_interval(job[\"start_time\"], time_lo, time_hi)\n",
    "        for assignment in job[\"detail\"]:\n",
    "            ip = assignment[\"ip\"]\n",
    "            gpus = assignment[\"gpus\"]\n",
    "            for gpu in gpus:\n",
    "                data_lists[0].append(\"{}_{}\".format(ip, gpu))\n",
    "                data_lists[1].append(\"gpu\")\n",
    "                data_lists[2].append(\"utilization\")\n",
    "\n",
    "                for idx, stime in enumerate(selected_times):\n",
    "                    if (stime, ip) in gpu_df.index:\n",
    "                        val = gpu_df.loc[(stime, ip), \"{}_util\".format(gpu)]\n",
    "                        if val is None:\n",
    "                            data_lists[3 + idx].append(None)\n",
    "                        else:\n",
    "                            data_lists[3 + idx].append(float(val))\n",
    "                    else:\n",
    "                        data_lists[3 + idx].append(None)\n",
    "        data_dict = dict()\n",
    "        for cname, cvalues in zip(output_columns, data_lists):\n",
    "            data_dict[cname] = cvalues\n",
    "        out_df = pd.DataFrame(data=data_dict)\n",
    "        out_df.to_csv(job_trace_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## per-job cpu trace collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bb422fef5a643beb149957758966038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "load cpu utilization traces:   0%|          | 0/45028260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cpu_df = philly_read_csv(os.path.join(trace_dir, \"cluster_cpu_util\"), 45028260, \"load cpu utilization traces\")\n",
    "cpu_df.set_index([\"time\", \"machine_id\"], inplace=True)\n",
    "cpu_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e272982a6d42409ba44a36854c80d417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "collect job cpu utilizations:   0%|          | 0/5910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for job in tqdm(sampled_jobs, desc='collect job cpu utilizations'):\n",
    "    job_trace_path = os.path.join(output_dir, \"cpu_util\", \"{}.csv\".format(job[\"jobid\"]))\n",
    "    if not os.path.isfile(job_trace_path):\n",
    "        data_lists = [list() for _ in output_columns]\n",
    "        selected_times = get_time_interval(job[\"start_time\"], time_lo, time_hi)\n",
    "        for assignment in job[\"detail\"]:\n",
    "            ip = assignment[\"ip\"]\n",
    "            data_lists[0].append(ip)\n",
    "            data_lists[1].append(\"cpu\")\n",
    "            data_lists[2].append(\"utilization\")\n",
    "\n",
    "            for idx, stime in enumerate(selected_times):\n",
    "                if (stime, ip) in cpu_df.index:\n",
    "                    val = cpu_df.loc[(stime, ip)].loc[(stime, ip), \"cpu_util\"]\n",
    "                    if val is None:\n",
    "                        data_lists[3 + idx].append(None)\n",
    "                    else:\n",
    "                        data_lists[3 + idx].append(val)\n",
    "                else:\n",
    "                    data_lists[3 + idx].append(None)\n",
    "        data_dict = dict()\n",
    "        for cname, cvalues in zip(output_columns, data_lists):\n",
    "            data_dict[cname] = cvalues\n",
    "        out_df = pd.DataFrame(data=data_dict)\n",
    "        out_df.to_csv(job_trace_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## per-job memory utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f6166140a7433ba7597d4258b922e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "load memory utilization traces:   0%|          | 0/45003060 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mem_df = philly_read_csv(os.path.join(trace_dir, \"cluster_mem_util\"), 45003060, \"load memory utilization traces\")\n",
    "mem_df.set_index([\"time\", \"machine_id\"], inplace=True)\n",
    "mem_df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15148c382ab04138ae5b09aa8ddac064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "collect job memory utilizations:   0%|          | 0/5910 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for job in tqdm(sampled_jobs, desc='collect job memory utilizations'):\n",
    "    job_trace_path = os.path.join(output_dir, \"mem_util\", \"{}.csv\".format(job[\"jobid\"]))\n",
    "    if not os.path.isfile(job_trace_path):\n",
    "        data_lists = [list() for _ in output_columns]\n",
    "        selected_times = get_time_interval(job[\"start_time\"], time_lo, time_hi)\n",
    "        for assignment in job[\"detail\"]:\n",
    "            ip = assignment[\"ip\"]\n",
    "            data_lists[0].append(ip)\n",
    "            data_lists[1].append(\"cpu\")\n",
    "            data_lists[2].append(\"memory\")\n",
    "\n",
    "            for idx, stime in enumerate(selected_times):\n",
    "                if (stime, ip) in mem_df.index:\n",
    "                    mtotal = mem_df.loc[(stime, ip), \"mem_total\"]\n",
    "                    mfree = mem_df.loc[(stime, ip), \"mem_free\"]\n",
    "                    if mtotal is None or mfree is None:\n",
    "                        data_lists[3 + idx].append(None)\n",
    "                    elif mtotal == -1 or mfree == -1:\n",
    "                        data_lists[3 + idx].append(-1)\n",
    "                    else:\n",
    "                        data_lists[3 + idx].append(100 * float(mfree) / float(mtotal))\n",
    "                else:\n",
    "                    data_lists[3 + idx].append(None)\n",
    "        data_dict = dict()\n",
    "        for cname, cvalues in zip(output_columns, data_lists):\n",
    "            data_dict[cname] = cvalues\n",
    "        out_df = pd.DataFrame(data=data_dict)\n",
    "        out_df.to_csv(job_trace_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3cb91839837c12cfcee7277f5c3320ca38bd1b0df530efe2c625284ea85332e0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('redeng')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
