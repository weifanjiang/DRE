{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = False\n",
    "clear_cache = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apricot\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import uuid\n",
    "import pickle\n",
    "import time\n",
    "import autosklearn.classification\n",
    "\n",
    "from CSSPy.volume_sampler import k_Volume_Sampling_Sampler\n",
    "from CSSPy.doublephase_sampler import double_Phase_Sampler\n",
    "from CSSPy.largest_leveragescores_sampler import largest_leveragescores_Sampler\n",
    "from CSSPy.dataset_tools import calculate_right_eigenvectors_k_svd\n",
    "from modAL.models import ActiveLearner\n",
    "from modAL.batch import uncertainty_batch_sampling\n",
    "from modAL.uncertainty import margin_sampling, entropy_sampling, uncertainty_sampling\n",
    "from modAL.expected_error import expected_error_reduction\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submodular_function_optimization(X, Y, **kwargs):\n",
    "    \"\"\"\n",
    "    Submodular function optimization for row/column sampling\n",
    "    does not require labeled dataset\n",
    "\n",
    "    Required params:\n",
    "    - model: fls or fbs\n",
    "    \"\"\"\n",
    "    if kwargs[\"dir\"] == 'col':\n",
    "        X = X.transpose()\n",
    "    if X.shape[0] == 1:\n",
    "        return [0, ]\n",
    "    tokeep = max(1, int(X.shape[0] * kwargs['keepFrac']))\n",
    "    if kwargs['model'] == 'fls':\n",
    "        clf = apricot.FacilityLocationSelection(tokeep).fit(X)\n",
    "    elif kwargs['model'] == 'fbs':\n",
    "        clf = apricot.FeatureBasedSelection(tokeep).fit(X)\n",
    "    else:\n",
    "        return None\n",
    "    toret = clf.ranking\n",
    "    selected = [int(i) for i in toret]\n",
    "    if len(selected) > tokeep:\n",
    "        selected = selected[:tokeep]\n",
    "    return selected\n",
    "\n",
    "\n",
    "def subset_selection_problem(X, Y, **kwargs):\n",
    "    \"\"\"\n",
    "    Subset selection to optimize for the span of selected rows/columns\n",
    "    does not require labeled dataset\n",
    "\n",
    "    Required params:\n",
    "    - sampler: volume, doublePhase or leverage\n",
    "    \"\"\"\n",
    "    if kwargs[\"dir\"] == 'row':\n",
    "        X = X.transpose()\n",
    "\n",
    "    if X.shape[0] <= 2 or X.shape[1] <= 2:\n",
    "        return [x for x in range(X.shape[1])]\n",
    "\n",
    "    tokeep = int(X.shape[0] * kwargs['keepFrac'])\n",
    "    tokeep = min(tokeep, X.shape[0] - 1)\n",
    "    tokeep = min(tokeep, X.shape[1] - 1)\n",
    "    tokeep = max(2, tokeep)\n",
    "\n",
    "    d = np.shape(X)[1]\n",
    "    N = np.shape(X)[0] - 1\n",
    "    # _, D, V = np.linalg.svd(X)\n",
    "    _, D, V = scipy.linalg.svd(X)\n",
    "    V_k = calculate_right_eigenvectors_k_svd(X, tokeep)\n",
    "\n",
    "    if kwargs['sampler'] == 'volume':\n",
    "        NAL = k_Volume_Sampling_Sampler(X, tokeep, D, V, d)\n",
    "        A_S = NAL.MultiRounds()\n",
    "    elif kwargs['sampler'] == 'doublePhase':\n",
    "        NAL = double_Phase_Sampler(X, tokeep, V_k, d, 10*tokeep)\n",
    "        A_S = NAL.DoublePhase()\n",
    "    elif kwargs['sampler'] == 'leverage':\n",
    "        NAL = largest_leveragescores_Sampler(X, tokeep, V, d)\n",
    "        A_S = NAL.MultiRounds()\n",
    "    \n",
    "    selected = [int(x) for x in NAL.selected]\n",
    "    if len(selected) > tokeep:\n",
    "        selected = selected[:tokeep]\n",
    "    return selected\n",
    "\n",
    "\n",
    "def active_learning(X, Y, **kwargs):\n",
    "    \"\"\"\n",
    "    Semi-supervised learning based sample selection\n",
    "    \"\"\"\n",
    "    selected = list()\n",
    "    tokeep = int(X.shape[0] * kwargs[\"keepFrac\"])\n",
    "    model = kwargs[\"model\"]\n",
    "    if model == \"RF\":\n",
    "        d2d = RandomForestClassifier()\n",
    "    elif model == \"MLP\":\n",
    "        d2d = MLPClassifier()\n",
    "    elif model == \"KNN\":\n",
    "        d2d = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "    # sample initial points\n",
    "    n_initial = max(int(kwargs[\"initFrac\"] * tokeep), 1)\n",
    "    initial_idx = np.random.choice(range(X.shape[0]), size=n_initial, replace=False)\n",
    "    selected.extend(initial_idx.tolist())\n",
    "    X_initial = X[initial_idx]\n",
    "    Y_initial = Y[initial_idx]\n",
    "    X_pool = np.delete(X, initial_idx, axis=0)\n",
    "    Y_pool = np.delete(Y, initial_idx, axis=0)\n",
    "\n",
    "    # construct learner\n",
    "    strat = kwargs[\"strat\"]\n",
    "    if strat == \"margin\":\n",
    "        samp = margin_sampling\n",
    "    elif strat == \"entropy\":\n",
    "        samp = entropy_sampling\n",
    "    elif strat == \"uncertain\":\n",
    "        samp = uncertainty_batch_sampling\n",
    "    elif strat == \"expected\":\n",
    "        samp = expected_error_reduction\n",
    "    learner = ActiveLearner(estimator=d2d, query_strategy=samp, X_training=X_initial, y_training=Y_initial)\n",
    "\n",
    "    n_queries = 10\n",
    "    n_instances = int((tokeep  - n_initial)/n_queries) + 1\n",
    "\n",
    "    for _ in range(n_queries):\n",
    "        query_idx, _ = learner.query(X_pool, n_instances=n_instances)\n",
    "        learner.teach(X=X_pool[query_idx], y=Y_pool[query_idx], only_new=True)\n",
    "        X_pool = np.delete(X_pool, query_idx, axis=0)\n",
    "        Y_pool = np.delete(Y_pool, query_idx, axis=0)\n",
    "        selected.extend(query_idx)\n",
    "    \n",
    "    return [int(x) for x in selected][:tokeep]\n",
    "\n",
    "\n",
    "def sampling_based_reduction(X, Y, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns list of selected col/row index\n",
    "    \"\"\"\n",
    "    mapper = {\n",
    "        \"smf\": submodular_function_optimization,\n",
    "        \"ssp\": subset_selection_problem,\n",
    "        \"al\": active_learning\n",
    "    }\n",
    "\n",
    "    return mapper[kwargs[\"method\"]](X, Y, **kwargs)\n",
    "\n",
    "\n",
    "# helper function for percentiles used in pandas\n",
    "# https://stackoverflow.com/questions/17578115/pass-percentiles-to-pandas-agg-function\n",
    "def get_percentile_fun(pct):\n",
    "\n",
    "    def percentile_(x):\n",
    "        return np.percentile(x, pct)\n",
    "    percentile_.__name__ = \"percentile_{}\".format(pct)\n",
    "\n",
    "    return percentile_\n",
    "\n",
    "\n",
    "def aggregation_based_reduction(X, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns aggregated matrix X\n",
    "    \"\"\"\n",
    "\n",
    "    # options for aggregation\n",
    "    # 1: only common aggregation functions\n",
    "    # 2: in addition to 1, add 4 percentiles\n",
    "    # 3: in addition to 1 & 2, add 2 percentiles\n",
    "    aggregation_options = {\n",
    "        # function set 1: common\n",
    "        1: [np.amax, np.amin, np.median, np.average, np.std],\n",
    "\n",
    "        # function set 2: percentiles\n",
    "        2: [10, 25, 75, 90],\n",
    "\n",
    "        # function set 3: more detailed percentiles\n",
    "        3: [1, 99]\n",
    "    }\n",
    "\n",
    "    aggregation_option_labels = {\n",
    "        1: [\"max\", \"min\", \"median\", \"avg\", \"std\"],\n",
    "        2: [\"pct10\", \"pct25\", \"pct75\", \"pct90\"],\n",
    "        3: [\"pct1\", \"pct99\"]\n",
    "    }\n",
    "\n",
    "    if kwargs[\"dir\"] == \"col\":\n",
    "        processed, processed_labels = list(), list()\n",
    "        for agg_option in range(1, kwargs[\"option\"] + 1):\n",
    "            funcs, func_labels = aggregation_options[agg_option], aggregation_option_labels[agg_option]\n",
    "            if agg_option == 1:\n",
    "                for func, func_label in zip(funcs, func_labels):\n",
    "                    agg_stat = func(X.values, axis=1)\n",
    "                    processed.append(agg_stat)\n",
    "                    processed_labels.append(func_label)\n",
    "            else:\n",
    "                agg_stat = np.percentile(a=X.values, q=funcs, axis=1).transpose()\n",
    "                processed.append(agg_stat)\n",
    "                processed_labels.extend(func_labels)\n",
    "        output_values = np.vstack(processed).transpose()\n",
    "        \n",
    "        if kwargs[\"prefix\"] is not None:\n",
    "            processed_labels = [kwargs[\"prefix\"] + \"_\" + x for x in processed_labels]\n",
    "        \n",
    "        return pd.DataFrame(data=output_values, columns=processed_labels)\n",
    "    \n",
    "    else:  # row aggregation\n",
    "        grb_criteria = kwargs[\"grb\"]\n",
    "        grb = X.groupby(by=grb_criteria)\n",
    "        functions_to_apply = list()\n",
    "        for agg_option in range(1, kwargs[\"option\"] + 1):\n",
    "            if agg_option == 1:\n",
    "                functions_to_apply.extend(aggregation_options[agg_option])\n",
    "            else:\n",
    "                functions_to_apply.extend([\n",
    "                    get_percentile_fun(x) for x in aggregation_options[agg_option]\n",
    "                ])\n",
    "        return grb.agg(functions_to_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scout_data_dir = \"data/scout\"\n",
    "scout_azure_dbfs_dir = \"/dbfs/user/weifan/\"\n",
    "scout_device_health_dir = \"device_health_data\"\n",
    "scout_azure_device_health_dir = os.path.join(scout_azure_dbfs_dir, scout_device_health_dir)\n",
    "scout_dummy_device_health_dir = os.path.join(scout_data_dir, scout_device_health_dir)\n",
    "scout_guided_reduction_save_dir = \"scout_guided_reduction\"\n",
    "scout_naive_reduction_save_dir = \"scout_unstructured_reduction\"\n",
    "scout_dummy_label_path = os.path.join(scout_data_dir, \"labels.csv\")\n",
    "scout_automl_evaluate_dir = \"scout_automl_evaluate\"\n",
    "scout_dummy_automl_eval_dir = os.path.join(scout_data_dir, scout_automl_evaluate_dir)\n",
    "scout_dbfs_automl_eval_dir = os.path.join(scout_azure_dbfs_dir, scout_automl_evaluate_dir)\n",
    "\n",
    "if os.path.isdir(scout_data_dir):\n",
    "    os.system(\"mkdir -p {}\".format(scout_dummy_automl_eval_dir))\n",
    "\n",
    "if os.path.isdir(scout_azure_dbfs_dir):\n",
    "    os.system(\"mkdir -p {}\".format(scout_dbfs_automl_eval_dir))\n",
    "\n",
    "scout_automl_time = 180\n",
    "scout_automl_mem = 30000\n",
    "\n",
    "scout_entity_types = ['cluster_switch', 'switch', 'tor_switch', ]\n",
    "scout_tiers = {\n",
    "    \"cluster_switch\": (0, 1),\n",
    "    \"switch\": (0, 1, 2, 3),\n",
    "    \"tor_switch\": (0, )\n",
    "}\n",
    "scout_metadata = ['IncidentId', 'EntityType', 'Tier']\n",
    "\n",
    "reduction_strengths = [0.2, 0.4, 0.6, 0.8]\n",
    "\n",
    "random.seed(10)\n",
    "np.random.seed(10)\n",
    "\n",
    "def load_raw_incident_device_health_reports(dummy=False):\n",
    "    if dummy:\n",
    "        source_dir = scout_dummy_device_health_dir\n",
    "        ret_df = pd.read_csv(\"data/scout/scout_anonymized_raw_data_compressed.gz\", index_col=0)\n",
    "        ret_df[[x for x in ret_df.columns if x not in scout_metadata]] = np.absolute(ret_df[[x for x in ret_df.columns if x not in scout_metadata]].values)\n",
    "        return ret_df\n",
    "    else:\n",
    "        source_dir = scout_azure_device_health_dir\n",
    "    \n",
    "    all_csv_files= [x for x in os.listdir(source_dir) if x.endswith(\".csv\")]\n",
    "    all_reports = list()\n",
    "    for fname in all_csv_files:\n",
    "        one_report = pd.read_csv(os.path.join(source_dir, fname))\n",
    "        all_reports.append(one_report)\n",
    "    \n",
    "    report_df = pd.concat(all_reports, axis=0)\n",
    "    dh_metric_cols = [x for x in report_df.columns[5:]]\n",
    "    report_df['Tier'] = report_df.apply(\n",
    "        lambda row: extract_tier_from_entity_name(row, dummy),\n",
    "        axis=1\n",
    "    )\n",
    "    report_df = report_df[report_df['Tier'].isin(['t0', 't1', 't2', 't3'])]\n",
    "    report_df.fillna(0, inplace=True)\n",
    "    min_metric_val_abs = abs(np.amin(report_df[dh_metric_cols].values))\n",
    "    report_df[dh_metric_cols] = report_df[dh_metric_cols].values + min_metric_val_abs\n",
    "    report_df = report_df[\n",
    "        [\"IncidentId\", \"EntityType\", \"Tier\",] + dh_metric_cols\n",
    "    ]\n",
    "    return report_df\n",
    "\n",
    "def extract_tier_from_entity_name(row, dummy=False):\n",
    "    if dummy:\n",
    "        return \"t{}\".format(np.random.choice(scout_tiers[row[\"EntityType\"]]))\n",
    "    \n",
    "    else:\n",
    "        # sample: dsm06-0102-0130-07t0\n",
    "        entity_name = row['EntityName']\n",
    "        return \"t\" + entity_name.split(\"-\")[-1].split('t')[-1]\n",
    "\n",
    "def get_str_desc_of_reduction_function(method_str, granularity, **kwargs):\n",
    "\n",
    "    if granularity is None:\n",
    "        gran_str = 'None'\n",
    "    elif type(granularity) == str:\n",
    "        gran_str = granularity\n",
    "    else:\n",
    "        gran_str = \"+\".join(granularity)\n",
    "\n",
    "    keys = sorted(list(kwargs.keys()))\n",
    "    vals_joined = \"-\".join([str(kwargs[x]) for x in keys])\n",
    "    return \"{}_{}_{}\".format(method_str, gran_str, vals_joined)\n",
    "\n",
    "def if_file_w_prefix_exists(dir, prefix):\n",
    "    existing_files = os.listdir(dir)\n",
    "    for e_file in existing_files:\n",
    "        if e_file.startswith(prefix):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def train_test_split_scout_data(raw_df, train_size):\n",
    "\n",
    "    all_incident_ids = raw_df.IncidentId.unique()\n",
    "    train_ids, test_ids = train_test_split(\n",
    "        all_incident_ids,\n",
    "        train_size=train_size,\n",
    "        random_state=10\n",
    "    )\n",
    "\n",
    "    train_ids, test_ids = set(train_ids), set(test_ids)\n",
    "\n",
    "    train_df = raw_df[raw_df.IncidentId.isin(train_ids)]\n",
    "    test_df = raw_df[raw_df.IncidentId.isin(test_ids)]\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def safe_get_subgroup(df_groupby, key):\n",
    "    if key in df_groupby.groups:\n",
    "        return df_groupby.get_group(key)\n",
    "    return None\n",
    "\n",
    "def scout_load_labels(df_list, dummy=False):\n",
    "\n",
    "    if dummy:\n",
    "        label_df = pd.read_csv(scout_dummy_label_path)\n",
    "        # generate random labels on the fly\n",
    "        generated = list()\n",
    "        for df in df_list:\n",
    "            generated.append(np.random.choice([0, 1], size=df.shape[0], replace=True))\n",
    "        return generated\n",
    "\n",
    "    else:\n",
    "        label_paths = [x for x in os.listdir(scout_azure_dbfs_dir) if x.startswith(\"sampled_incidents_\")]\n",
    "        label_paths = [x for x in label_paths if x.endswith(\".csv\")]\n",
    "\n",
    "        loaded = list()\n",
    "        for label_path in label_paths:\n",
    "            loaded.append(pd.read_csv(os.path.join(scout_azure_dbfs_dir, label_path))[['IncidentId', 'Label']])\n",
    "        label_df = pd.concat(loaded, axis=0).reset_index(drop=True).drop_duplicates(ignore_index=True)\n",
    "    \n",
    "    label_dict = dict()\n",
    "    for _, row in label_df.iterrows():\n",
    "        label_dict[row[\"IncidentId\"]] = int(row[\"Label\"])\n",
    "    \n",
    "    extracted_labels = list()\n",
    "    for df in df_list:\n",
    "        extracted_labels.append(np.array([label_dict.get(x, -1) for x in df.IncidentId.values]))\n",
    "\n",
    "    return extracted_labels\n",
    "\n",
    "\n",
    "def convert_severity_level_to_label(severity):\n",
    "    severity = int(severity)\n",
    "    if severity <= 3:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def scout_load_severity(df_list, dummy=False):\n",
    "    if dummy:\n",
    "        return None\n",
    "    else:\n",
    "        severity_fpath = os.path.join(scout_azure_dbfs_dir, \"incident_severity.csv\")\n",
    "        label_df = pd.read_csv(severity_fpath)\n",
    "        label_df['Label'] = label_df.apply(lambda row: convert_severity_level_to_label(row['severity'], axis=1))\n",
    "    \n",
    "    label_dict = dict()\n",
    "    for _, row in label_df.iterrows():\n",
    "        label_dict[row[\"IncidentId\"]] = int(row[\"Label\"])\n",
    "    \n",
    "    extracted_labels = list()\n",
    "    for df in df_list:\n",
    "        extracted_labels.append(np.array([label_dict.get(x, -1) for x in df.IncidentId.values]))\n",
    "\n",
    "    return extracted_labels\n",
    "\n",
    "\n",
    "def add_missing_cols_to_test(train_df, test_df):\n",
    "    missing_cols = [x for x in train_df.columns if x not in test_df.columns]\n",
    "    for missing_col in missing_cols:\n",
    "        test_df[missing_col] = None\n",
    "    test_df = test_df[train_df.columns]\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scout_evaluator(scout_savepath, output_save_path=None, objective='incidentRouting'):\n",
    "\n",
    "    with open(scout_savepath, \"rb\") as fin:\n",
    "        train_df, test_df = pickle.load(fin)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    val_cols = [x for x in train_df.columns if x not in scout_metadata]\n",
    "    \n",
    "    num_row, num_col = int(train_df[val_cols].shape[0]), int(train_df[val_cols].shape[1])\n",
    "    num_null = int(train_df[val_cols].isna().values.sum())\n",
    "    \n",
    "    test_df = add_missing_cols_to_test(train_df, test_df)\n",
    "\n",
    "    train_df.fillna(0, inplace=True)\n",
    "    test_df.fillna(0, inplace=True)\n",
    "    \n",
    "    # check if Row Aggregation is applied\n",
    "    if 'RowAgg' not in os.path.basename(scout_savepath):\n",
    "        # not applied, apply row aggregation\n",
    "        # with No granularity\n",
    "        # and 1 as aggregation function option\n",
    "\n",
    "        train_test_processed = list()\n",
    "        grb_cols = ['IncidentId', ] + [x for x in train_df.columns if x not in scout_metadata]\n",
    "        for raw_df in [train_df, test_df,]:\n",
    "            aggregated_result = aggregation_based_reduction(raw_df[grb_cols], dir=\"row\", grb='IncidentId', option=1)\n",
    "            rename_col = list()\n",
    "            for old_col in aggregated_result.columns:\n",
    "                rename_col.append(\":\".join(old_col))\n",
    "            aggregated_result.columns = rename_col\n",
    "            aggregated_result.reset_index(inplace=True)\n",
    "            metrics_left = [x for x in aggregated_result.columns if x not in scout_metadata]\n",
    "            aggregated_result = aggregated_result[['IncidentId', ] + metrics_left]\n",
    "            train_test_processed.append(aggregated_result)\n",
    "        \n",
    "        train_df, test_df = train_test_processed\n",
    "        metadata_gran = None\n",
    "    \n",
    "    else:\n",
    "        # applied\n",
    "        metadata_gran = os.path.basename(scout_savepath).split('RowAgg_')[1].split(\"_\")[0]\n",
    "        if metadata_gran == 'None':\n",
    "            metadata_gran = None\n",
    "        elif \"+\" in metadata_gran:\n",
    "            metadata_gran = metadata_gran.split(\"+\")\n",
    "\n",
    "    if metadata_gran is None:\n",
    "        train_data_vectors, test_data_vectors = train_df, test_df\n",
    "    else:\n",
    "        vectorized = list()\n",
    "        for data_df in [train_df, test_df]:\n",
    "            grb_gran = data_df.groupby(metadata_gran)\n",
    "            renamed_df = None\n",
    "            for key, sub_df in grb_gran:\n",
    "\n",
    "                if type(key) == str:\n",
    "                    suffix = key\n",
    "                else:\n",
    "                    suffix = \"+\".join(key)\n",
    "\n",
    "                new_col_names = list()\n",
    "                for col_name in sub_df.columns:\n",
    "                    if col_name in scout_metadata:\n",
    "                        new_col_names.append(col_name)\n",
    "                    else:\n",
    "                        new_col_names.append(\"{}({})\".format(col_name, suffix))\n",
    "                \n",
    "                sub_df.columns = new_col_names\n",
    "                sub_df = sub_df[['IncidentId',] + [x for x in new_col_names if x not in scout_metadata]]\n",
    "                if renamed_df is None:\n",
    "                    renamed_df = sub_df\n",
    "                else:\n",
    "                    renamed_df = renamed_df.merge(sub_df, how='left', on='IncidentId')\n",
    "            vectorized.append(renamed_df)\n",
    "    \n",
    "        train_incident_ids = set(train_df.IncidentId.values)\n",
    "        test_incident_ids = set(test_df.IncidentId.values)\n",
    "\n",
    "        vectorized_df = pd.concat(vectorized, axis=0)\n",
    "        train_data_vectors = vectorized_df[vectorized_df.IncidentId.isin(train_incident_ids)]\n",
    "        test_data_vectors = vectorized_df[vectorized_df.IncidentId.isin(test_incident_ids)]\n",
    "    \n",
    "    if objective == 'incidentRouting':\n",
    "        train_label, test_label = scout_load_labels([train_data_vectors, test_data_vectors], dummy=dummy)\n",
    "    elif objective == 'severityPrediction':\n",
    "        train_label, test_label = scout_load_severity([train_data_vectors, test_data_vectors], dummy=dummy)\n",
    "\n",
    "    pred_columns = [x for x in train_data_vectors.columns if x not in scout_metadata]\n",
    "\n",
    "    # train AutoML evaluator with fixed budget\n",
    "    train_vals = train_data_vectors[pred_columns].values\n",
    "    test_vals = test_data_vectors[pred_columns].values\n",
    "\n",
    "    # in case if there are missing labels\n",
    "    train_vals, train_label = train_vals[train_label != -1], train_label[train_label != -1]\n",
    "    test_vals, test_label = test_vals[test_label != -1], test_label[test_label != -1]\n",
    "\n",
    "    automl = autosklearn.classification.AutoSklearnClassifier(\n",
    "        time_left_for_this_task=scout_automl_time,\n",
    "        n_jobs=8,\n",
    "        per_run_time_limit=scout_automl_time//6,\n",
    "        memory_limit=scout_automl_mem\n",
    "    )\n",
    "\n",
    "    automl.fit(train_vals, train_label)\n",
    "    test_pred = automl.predict(test_vals).astype(int)\n",
    "\n",
    "    eval_result = {\n",
    "        \"num_row\": num_row,\n",
    "        \"num_col\": num_col,\n",
    "        \"num_null\": num_null,\n",
    "        \"accuracy\": accuracy_score(test_label, test_pred),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(test_label, test_pred),\n",
    "        \"f1\": f1_score(test_label, test_pred),\n",
    "        \"precision\": precision_score(test_label, test_pred),\n",
    "        \"recall\": recall_score(test_label, test_pred),\n",
    "        \"eval_time\": float(time.time() - start_time)\n",
    "    }\n",
    "    \n",
    "    if output_save_path is not None:\n",
    "        with open(output_save_path, \"w\") as fout:\n",
    "            json.dump(eval_result, fout, indent=2)\n",
    "    else:\n",
    "        return eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CLEAR CACHES #####\n",
    "if clear_cache:\n",
    "    os.system('rm -rf {}'.format(scout_automl_evaluate_dir))\n",
    "    os.system('rm -rf {}'.format(os.path.join(scout_azure_dbfs_dir, scout_naive_reduction_save_dir)))\n",
    "    os.system('rm -rf {}'.format(os.path.join(scout_azure_dbfs_dir, scout_guided_reduction_save_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SCOUT NAIVE REDUCTIONS #####\n",
    "\n",
    "save_dir = scout_naive_reduction_save_dir\n",
    "\n",
    "scout_raw_df = load_raw_incident_device_health_reports(dummy=dummy)\n",
    "train_df, test_df = train_test_split_scout_data(scout_raw_df, 0.8)\n",
    "\n",
    "if dummy:\n",
    "    one_hop_out_dir = os.path.join(scout_data_dir, save_dir, \"one_hop\")\n",
    "else:\n",
    "    one_hop_out_dir = os.path.join(scout_azure_dbfs_dir, save_dir, \"one_hop\")\n",
    "os.system(\"mkdir -p {}\".format(one_hop_out_dir))\n",
    "\n",
    "metric_cols = [x for x in train_df.columns if x not in scout_metadata]\n",
    "\n",
    "# Sampling based: SMF FLS COL, SMF FBS COL & ROW\n",
    "# SSP not feasible yet: unable to perform SVD on too many rows\n",
    "for keepFrac in reduction_strengths:\n",
    "    # for technique in [\"RowSampling\", \"ColSampling\"]:\n",
    "    for technique in [\"ColSampling\", ]:\n",
    "        dir = technique[:3].lower()\n",
    "        for model in [\"fls\", \"fbs\"]:\n",
    "            if model == 'fls' and dir == 'row':\n",
    "                continue\n",
    "            else:\n",
    "                str_desc = get_str_desc_of_reduction_function(technique, None, method=\"smf\", dir=dir, model=model, keepFrac=keepFrac)\n",
    "                print(str_desc)\n",
    "                if not if_file_w_prefix_exists(one_hop_out_dir, str_desc):\n",
    "                    start_time = time.time()\n",
    "                    selected_idx = sampling_based_reduction(\n",
    "                        train_df[metric_cols].values,\n",
    "                        None,\n",
    "                        method='smf',\n",
    "                        dir=dir,\n",
    "                        model=model,\n",
    "                        keepFrac=keepFrac\n",
    "                    )\n",
    "                    end_time = time.time()\n",
    "                    time_taken = round(end_time - start_time, 5)\n",
    "                    save_file_name = os.path.join(one_hop_out_dir, \"{}_sec{}.pickle\".format(str_desc, time_taken))\n",
    "                    if dir == 'col':\n",
    "                        selected_columns = [metric_cols[x] for x in selected_idx]\n",
    "                        train_save = train_df[scout_metadata + selected_columns]\n",
    "                        test_save = test_df[scout_metadata + selected_columns]\n",
    "                    else:\n",
    "                        train_save = train_df.iloc[selected_idx]\n",
    "                        test_save = test_df\n",
    "                    with open(save_file_name, \"wb\") as fout:\n",
    "                        pickle.dump((train_save, test_save), fout)\n",
    "\n",
    "\n",
    "# Row aggregation\n",
    "agg_cols = ['IncidentId', ] + [x for x in train_df.columns if x not in scout_metadata]\n",
    "# for option in [1, 2, 3]:\n",
    "for option in [1, ]:\n",
    "    str_desc = get_str_desc_of_reduction_function(\"RowAgg\", None, dir=\"row\", grb=\"IncidentId\", option=option)\n",
    "    print(str_desc)\n",
    "    if not if_file_w_prefix_exists(one_hop_out_dir, str_desc):\n",
    "        start_time = time.time()\n",
    "        train_agg_result = aggregation_based_reduction(train_df[agg_cols], dir='row', grb='IncidentId', option=option)\n",
    "        end_time = time.time()\n",
    "        time_taken = round(end_time - start_time, 5)\n",
    "        test_agg_result = aggregation_based_reduction(test_df[agg_cols], dir='row', grb='IncidentId', option=option)\n",
    "    \n",
    "        renamed_dfs = list()\n",
    "        for df in [train_agg_result, test_agg_result]:\n",
    "            rename_cols = [\":\".join(x) for x in df.columns]\n",
    "            df.columns = rename_cols\n",
    "            df.reset_index(inplace=True)\n",
    "            renamed_dfs.append(df)\n",
    "        \n",
    "        train_save, test_save = renamed_dfs\n",
    "        save_file_name = os.path.join(one_hop_out_dir, \"{}_sec{}.pickle\".format(str_desc, time_taken))\n",
    "        with open(save_file_name, \"wb\") as fout:\n",
    "            pickle.dump((train_save, test_save), fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### EVALUATE SCOUT NAIVE REDUCTIONS #####\n",
    "if dummy:\n",
    "    naive_save_dir = os.path.join(scout_data_dir, scout_naive_reduction_save_dir, \"one_hop\")\n",
    "    eval_dir = scout_dummy_automl_eval_dir\n",
    "else:\n",
    "    naive_save_dir = os.path.join(scout_azure_dbfs_dir, scout_naive_reduction_save_dir, \"one_hop\")\n",
    "    eval_dir = scout_dbfs_automl_eval_dir\n",
    "os.system(\"mkdir -p {}\".format(eval_dir))\n",
    "\n",
    "reduced_instances = [x for x in os.listdir(naive_save_dir) if x.endswith(\".pickle\")]\n",
    "for red in reduced_instances:\n",
    "    out_path = os.path.join(eval_dir, red.replace(\"pickle\", \"json\"))\n",
    "    print(out_path)\n",
    "    if not os.path.isfile(out_path):\n",
    "        scout_evaluator(os.path.join(naive_save_dir, red), out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DIFFERENT DATASET, SAME PURPOSE #####\n",
    "# apply naive reductions to column-splitted scout dataset\n",
    "\n",
    "save_dir = scout_naive_reduction_save_dir\n",
    "\n",
    "scout_raw_df = load_raw_incident_device_health_reports(dummy=dummy)\n",
    "train_df_raw, test_df_raw = train_test_split_scout_data(scout_raw_df, 0.8)\n",
    "\n",
    "all_metric_cols = [x for x in train_df_raw.columns if x not in scout_metadata]\n",
    "sample_size = len(all_metric_cols) // 2\n",
    "metrics_0 = list(np.random.choice(all_metric_cols, size=sample_size, replace=False))\n",
    "metrics_1 = [x for x in all_metric_cols if x not in metrics_0]\n",
    "\n",
    "metrics = [metrics_0, metrics_1]\n",
    "\n",
    "for idx in range(2):\n",
    "\n",
    "    metric_cols = metrics[idx]\n",
    "    train_df = train_df_raw[scout_metadata + metric_cols]\n",
    "    test_df = test_df_raw[scout_metadata + metric_cols]\n",
    "\n",
    "    if dummy:\n",
    "        one_hop_out_dir = os.path.join(scout_data_dir, save_dir, \"one_hop_metricSet{}\".format(idx))\n",
    "    else:\n",
    "        one_hop_out_dir = os.path.join(scout_azure_dbfs_dir, save_dir, \"one_hop_metricSet{}\".format(idx))\n",
    "    os.system(\"mkdir -p {}\".format(one_hop_out_dir))\n",
    "\n",
    "    metric_cols = [x for x in train_df.columns if x not in scout_metadata]\n",
    "\n",
    "    # Sampling based: SMF FLS COL, SMF FBS COL & ROW\n",
    "    # SSP not feasible yet: unable to perform SVD on too many rows\n",
    "    for keepFrac in reduction_strengths:\n",
    "        # for technique in [\"RowSampling\", \"ColSampling\"]:\n",
    "        for technique in [\"ColSampling\", ]:\n",
    "            dir = technique[:3].lower()\n",
    "            for model in [\"fls\", \"fbs\"]:\n",
    "                if model == 'fls' and dir == 'row':\n",
    "                    continue\n",
    "                else:\n",
    "                    str_desc = get_str_desc_of_reduction_function(technique, None, method=\"smf\", dir=dir, model=model, keepFrac=keepFrac)\n",
    "                    print(str_desc)\n",
    "                    if not if_file_w_prefix_exists(one_hop_out_dir, str_desc):\n",
    "                        start_time = time.time()\n",
    "                        selected_idx = sampling_based_reduction(\n",
    "                            train_df[metric_cols].values,\n",
    "                            None,\n",
    "                            method='smf',\n",
    "                            dir=dir,\n",
    "                            model=model,\n",
    "                            keepFrac=keepFrac\n",
    "                        )\n",
    "                        end_time = time.time()\n",
    "                        time_taken = round(end_time - start_time, 5)\n",
    "                        save_file_name = os.path.join(one_hop_out_dir, \"{}_sec{}.pickle\".format(str_desc, time_taken))\n",
    "                        if dir == 'col':\n",
    "                            selected_columns = [metric_cols[x] for x in selected_idx]\n",
    "                            train_save = train_df[scout_metadata + selected_columns]\n",
    "                            test_save = test_df[scout_metadata + selected_columns]\n",
    "                        else:\n",
    "                            train_save = train_df.iloc[selected_idx]\n",
    "                            test_save = test_df\n",
    "                        with open(save_file_name, \"wb\") as fout:\n",
    "                            pickle.dump((train_save, test_save), fout)\n",
    "\n",
    "\n",
    "    # Row aggregation\n",
    "    agg_cols = ['IncidentId', ] + [x for x in train_df.columns if x not in scout_metadata]\n",
    "    # for option in [1, 2, 3]:\n",
    "    for option in [1, ]:\n",
    "        str_desc = get_str_desc_of_reduction_function(\"RowAgg\", None, dir=\"row\", grb=\"IncidentId\", option=option)\n",
    "        print(str_desc)\n",
    "        if not if_file_w_prefix_exists(one_hop_out_dir, str_desc):\n",
    "            start_time = time.time()\n",
    "            train_agg_result = aggregation_based_reduction(train_df[agg_cols], dir='row', grb='IncidentId', option=option)\n",
    "            end_time = time.time()\n",
    "            time_taken = round(end_time - start_time, 5)\n",
    "            test_agg_result = aggregation_based_reduction(test_df[agg_cols], dir='row', grb='IncidentId', option=option)\n",
    "        \n",
    "            renamed_dfs = list()\n",
    "            for df in [train_agg_result, test_agg_result]:\n",
    "                rename_cols = [\":\".join(x) for x in df.columns]\n",
    "                df.columns = rename_cols\n",
    "                df.reset_index(inplace=True)\n",
    "                renamed_dfs.append(df)\n",
    "            \n",
    "            train_save, test_save = renamed_dfs\n",
    "            save_file_name = os.path.join(one_hop_out_dir, \"{}_sec{}.pickle\".format(str_desc, time_taken))\n",
    "            with open(save_file_name, \"wb\") as fout:\n",
    "                pickle.dump((train_save, test_save), fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### EVALUATE DIFFERENT DATASET, SAME PURPOSE #####\n",
    "for idx in range(2):\n",
    "    if dummy:\n",
    "        naive_save_dir = os.path.join(scout_data_dir, scout_naive_reduction_save_dir, \"one_hop_metricSet{}\".format(idx))\n",
    "        eval_dir = os.path.join(scout_dummy_automl_eval_dir, \"metricSet{}\".format(idx))\n",
    "    else:\n",
    "        naive_save_dir = os.path.join(scout_azure_dbfs_dir, scout_naive_reduction_save_dir, \"one_hop_metricSet{}\".format(idx))\n",
    "        eval_dir = os.path.join(scout_dbfs_automl_eval_dir, \"metricSet{}\".format(idx))\n",
    "    os.system(\"mkdir -p {}\".format(eval_dir))\n",
    "\n",
    "    reduced_instances = [x for x in os.listdir(naive_save_dir) if x.endswith(\".pickle\")]\n",
    "    for red in reduced_instances:\n",
    "        out_path = os.path.join(eval_dir, red.replace(\"pickle\", \"json\"))\n",
    "        print(out_path)\n",
    "        if not os.path.isfile(out_path):\n",
    "            scout_evaluator(os.path.join(naive_save_dir, red), out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SCOUT GRAMMAR GUIDED ONE-HOP REDUCTION #####\n",
    "save_dir = scout_guided_reduction_save_dir\n",
    "scout_raw_df = load_raw_incident_device_health_reports(dummy=dummy)\n",
    "train_df, test_df = train_test_split_scout_data(scout_raw_df, 0.8)\n",
    "granularities = ['EntityType', 'Tier', ['EntityType', 'Tier']]\n",
    "\n",
    "if dummy:\n",
    "    one_hop_out_dir = os.path.join(scout_data_dir, save_dir, \"one_hop\")\n",
    "else:\n",
    "    one_hop_out_dir = os.path.join(scout_azure_dbfs_dir, save_dir, \"one_hop\")\n",
    "os.system(\"mkdir -p {}\".format(one_hop_out_dir))\n",
    "\n",
    "for granularity in granularities:\n",
    "\n",
    "    grb_gran = train_df.groupby(granularity)\n",
    "    grb_gran_test = test_df.groupby(granularity)\n",
    "\n",
    "    # sampling based\n",
    "    cols_to_sample = [x for x in train_df.columns if x not in scout_metadata]\n",
    "    for keepFrac in reduction_strengths:\n",
    "        # for technique in [\"RowSampling\", \"ColSampling\"]:\n",
    "        for technique in [\"ColSampling\", ]:\n",
    "            method = \"smf\"\n",
    "            dir = technique[:3].lower()\n",
    "            for model in [\"fls\", \"fbs\"]:\n",
    "                if model == 'fls' and dir == 'row':\n",
    "                    continue\n",
    "                else:\n",
    "                    str_desc = get_str_desc_of_reduction_function(technique, granularity, method=method, dir=\"col\", model=model, keepFrac=keepFrac)\n",
    "                    print(str_desc)\n",
    "                    if not if_file_w_prefix_exists(one_hop_out_dir, str_desc):\n",
    "                        processed = list()\n",
    "                        processed_test = list()\n",
    "                        time_taken = 0\n",
    "                        for key, sub_df in grb_gran:\n",
    "                            start_time = time.time()\n",
    "                            selected_idx = sampling_based_reduction(\n",
    "                                sub_df[cols_to_sample].values,\n",
    "                                None,\n",
    "                                method=method,\n",
    "                                dir=dir,\n",
    "                                model=model,\n",
    "                                keepFrac=keepFrac\n",
    "                            )\n",
    "                            end_time = time.time()\n",
    "                            time_taken += end_time - start_time\n",
    "                            sub_df_test = safe_get_subgroup(grb_gran_test, key)\n",
    "\n",
    "                            if dir == \"col\":\n",
    "                                selected_columns = [cols_to_sample[x] for x in selected_idx]\n",
    "                                processed.append(sub_df[scout_metadata + selected_columns])\n",
    "                                if sub_df_test is not None:\n",
    "                                    processed_test.append(sub_df_test[scout_metadata + selected_columns])\n",
    "                            else:\n",
    "                                processed.append(sub_df.iloc[selected_idx])\n",
    "                                if sub_df_test is not None:\n",
    "                                    processed_test.append(sub_df_test)\n",
    "\n",
    "                        time_taken = round(time_taken, 5)\n",
    "                        save_file_name = os.path.join(one_hop_out_dir, \"{}_sec{}.pickle\".format(str_desc, time_taken))\n",
    "                        to_save = pd.concat(processed, axis=0, ignore_index=True)\n",
    "                        to_save_test = pd.concat(processed_test, axis=0, ignore_index=True)\n",
    "                        with open(save_file_name, \"wb\") as fout:\n",
    "                            pickle.dump((to_save, to_save_test), fout)\n",
    "    \n",
    "    # row aggregation\n",
    "    grb_cols = [x for x in train_df.columns if x not in scout_metadata]\n",
    "    grb_cols = ['IncidentId', ] + grb_cols\n",
    "    # for option in [1, 2, 3, ]:\n",
    "    for option in [1, ]:\n",
    "        str_desc = get_str_desc_of_reduction_function(\"RowAgg\", granularity, dir=\"row\", grb=\"IncidentId\", option=option)\n",
    "        print(str_desc)\n",
    "\n",
    "        # check if file exists in directory\n",
    "        if not if_file_w_prefix_exists(one_hop_out_dir, str_desc):\n",
    "            processed, processed_test = list(), list()\n",
    "            time_taken = 0\n",
    "    \n",
    "            for keys, sub_df in grb_gran:\n",
    "                start_time = time.time()\n",
    "                aggregated_result = aggregation_based_reduction(sub_df[grb_cols], dir=\"row\", grb='IncidentId', option=option)\n",
    "                time_taken += time.time() - start_time\n",
    "\n",
    "                rename_col = list()\n",
    "                for old_col in aggregated_result.columns:\n",
    "                    rename_col.append(\":\".join(old_col))\n",
    "                \n",
    "                aggregated_result.columns = rename_col\n",
    "                aggregated_result.reset_index(inplace=True)\n",
    "\n",
    "                if type(granularity) != str:\n",
    "                    for gran_name, gran_val in zip(granularity, keys):\n",
    "                        aggregated_result[gran_name] = gran_val\n",
    "                else:\n",
    "                    aggregated_result[granularity] = keys\n",
    "\n",
    "                processed.append(aggregated_result)\n",
    "\n",
    "                sub_df_test = safe_get_subgroup(grb_gran_test, keys)\n",
    "                if sub_df_test is not None:\n",
    "                    aggregated_result_test = aggregation_based_reduction(\n",
    "                        sub_df_test[grb_cols], dir=\"row\", grb='IncidentId', option=option\n",
    "                    )\n",
    "                    rename_col_test = list()\n",
    "                    for old_col in aggregated_result_test.columns:\n",
    "                        rename_col_test.append(\":\".join(old_col))\n",
    "                    aggregated_result_test.columns = rename_col\n",
    "                    aggregated_result_test.reset_index(inplace=True)\n",
    "\n",
    "                    if type(granularity) != str:\n",
    "                        for gran_name, gran_val in zip(granularity, keys):\n",
    "                            aggregated_result_test[gran_name] = gran_val\n",
    "                    else:\n",
    "                        aggregated_result_test[granularity] = keys\n",
    "                    \n",
    "                    processed_test.append(aggregated_result_test)\n",
    "\n",
    "            to_save = pd.concat(processed, axis=0, ignore_index=True)\n",
    "            to_save_test = pd.concat(processed_test, axis=0, ignore_index=True)\n",
    "\n",
    "            # reorganize columns\n",
    "            metadata_left = [x for x in scout_metadata if x in granularity]\n",
    "            metrics_left = [x for x in to_save.columns if x not in scout_metadata]\n",
    "            out_columns = ['IncidentId', ] + metadata_left + metrics_left\n",
    "\n",
    "            time_taken = round(time_taken, 5)\n",
    "            save_file_name = os.path.join(one_hop_out_dir, \"{}_sec{}.pickle\".format(str_desc, time_taken))\n",
    "            to_save, to_save_test = to_save[out_columns], to_save_test[out_columns]\n",
    "\n",
    "            with open(save_file_name, \"wb\") as fout:\n",
    "                pickle.dump((to_save, to_save_test), fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### EVALUATE SCOUT GUIDED ONE-HOP REDUCTIONS #####\n",
    "if dummy:\n",
    "    guided_one_hop_save_dir = os.path.join(scout_data_dir, scout_guided_reduction_save_dir, \"one_hop\")\n",
    "    eval_dir = scout_dummy_automl_eval_dir\n",
    "else:\n",
    "    guided_one_hop_save_dir = os.path.join(scout_azure_dbfs_dir, scout_guided_reduction_save_dir, \"one_hop\")\n",
    "    eval_dir = scout_dbfs_automl_eval_dir\n",
    "os.system(\"mkdir -p {}\".format(eval_dir))\n",
    "\n",
    "reduced_instances = [x for x in os.listdir(guided_one_hop_save_dir) if x.endswith(\".pickle\")]\n",
    "for red in reduced_instances:\n",
    "    out_path = os.path.join(eval_dir, red.replace(\"pickle\", \"json\"))\n",
    "    print(out_path)\n",
    "    if not os.path.isfile(out_path):\n",
    "        scout_evaluator(os.path.join(guided_one_hop_save_dir, red), out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SCOUT GRAMMAR GUIDED TWO-HOP REDUCTION #####\n",
    "save_dir = scout_guided_reduction_save_dir\n",
    "if dummy:\n",
    "    one_hop_out_dir = os.path.join(scout_data_dir, save_dir, \"one_hop\")\n",
    "    two_hop_out_dir = os.path.join(scout_data_dir, save_dir, \"two_hop\")\n",
    "else:\n",
    "    one_hop_out_dir = os.path.join(scout_azure_dbfs_dir, save_dir, \"one_hop\")\n",
    "    two_hop_out_dir = os.path.join(scout_azure_dbfs_dir, save_dir, \"two_hop\")\n",
    "os.system('mkdir -p {}'.format(two_hop_out_dir))\n",
    "one_hop_filepaths = [x for x in os.listdir(one_hop_out_dir) if x.endswith(\".pickle\")]\n",
    "\n",
    "for one_hop_filepath in one_hop_filepaths:\n",
    "    one_hop_str = one_hop_filepath.split(\".pickle\")[0]\n",
    "    prev_algo = one_hop_str.split(\"_\")[0]\n",
    "    with open(os.path.join(one_hop_out_dir, one_hop_filepath), \"rb\") as fin:\n",
    "        train_df, test_df = pickle.load(fin)\n",
    "    test_df = add_missing_cols_to_test(train_df, test_df)\n",
    "    train_df.fillna(0, inplace=True)\n",
    "    test_df.fillna(0, inplace=True)\n",
    "    existing_metadatas = [x for x in scout_metadata if x in train_df.columns]\n",
    "\n",
    "    granularity = one_hop_filepath.split(\"_\")[1]\n",
    "    if \"+\" in granularity:\n",
    "        granularity = granularity.split(\"+\")\n",
    "    \n",
    "    grb_gran = train_df.groupby(granularity)\n",
    "    grb_gran_test = test_df.groupby(granularity)\n",
    "    \n",
    "    # attempt to apply SMF column sampling, row sampling, and row aggregation as second-hop\n",
    "    existing_metrics = [x for x in train_df.columns if x not in scout_metadata]\n",
    "    if prev_algo != 'ColSampling':\n",
    "        for keepFrac in reduction_strengths:\n",
    "            # SMF\n",
    "            for model in [\"fls\", \"fbs\"]:\n",
    "                str_desc = get_str_desc_of_reduction_function('ColSampling', granularity, method='smf', dir='col', model=model, keepFrac=keepFrac)\n",
    "                two_hop_desc = one_hop_str + \"&\" + str_desc\n",
    "                print(two_hop_desc)\n",
    "                if not if_file_w_prefix_exists(two_hop_out_dir, two_hop_desc):\n",
    "                    processed, processed_test = list(), list()\n",
    "                    time_taken = 0\n",
    "                    for key, sub_df in grb_gran:\n",
    "                        start_time = time.time()\n",
    "                        selected_idx = sampling_based_reduction(\n",
    "                            sub_df[existing_metrics].values,\n",
    "                            None,\n",
    "                            method='smf',\n",
    "                            dir='col',\n",
    "                            model=model,\n",
    "                            keepFrac=keepFrac\n",
    "                        )\n",
    "                        end_time = time.time()\n",
    "                        time_taken += end_time - start_time\n",
    "                        selected_columns = [existing_metrics[x] for x in selected_idx]\n",
    "                        processed.append(sub_df[existing_metadatas + selected_columns])\n",
    "                        sub_df_test = safe_get_subgroup(grb_gran_test, key)\n",
    "                        if sub_df_test is not None:\n",
    "                            processed_test.append(sub_df_test[existing_metadatas + selected_columns])\n",
    "                    time_taken = round(time_taken, 5)\n",
    "                    save_file_name = os.path.join(two_hop_out_dir, \"{}_sec{}.pickle\".format(two_hop_desc, time_taken))\n",
    "                    to_save = pd.concat(processed, axis=0, ignore_index=True)\n",
    "                    to_save_test = pd.concat(processed_test, axis=0, ignore_index=True)\n",
    "                    with open(save_file_name, \"wb\") as fout:\n",
    "                        pickle.dump((to_save, to_save_test), fout)\n",
    "            # SSP only if row aggregation is previously applied\n",
    "            if prev_algo == 'RowAgg':\n",
    "                for sampler in ['doublePhase', 'leverage']:\n",
    "                    str_desc = get_str_desc_of_reduction_function('ColSampling', granularity, method='ssp', dir='col', sampler=sampler, keepFrac=keepFrac)\n",
    "                    two_hop_desc = one_hop_str + \"&\" + str_desc\n",
    "                    print(two_hop_desc)\n",
    "                    if not if_file_w_prefix_exists(two_hop_out_dir, two_hop_desc):\n",
    "                        processed, processed_test = list(), list()\n",
    "                        time_taken = 0\n",
    "                        for key, sub_df in grb_gran:\n",
    "                            start_time = time.time()\n",
    "                            selected_idx = sampling_based_reduction(\n",
    "                                sub_df[existing_metrics].values,\n",
    "                                None,\n",
    "                                method='ssp',\n",
    "                                dir='col',\n",
    "                                sampler=sampler,\n",
    "                                keepFrac=keepFrac\n",
    "                            )\n",
    "                            end_time = time.time()\n",
    "                            time_taken += end_time - start_time\n",
    "                            selected_columns = [existing_metrics[x] for x in selected_idx]\n",
    "                            processed.append(sub_df[existing_metadatas + selected_columns])\n",
    "                            sub_df_test = safe_get_subgroup(grb_gran_test, key)\n",
    "                            if sub_df_test is not None:\n",
    "                                processed_test.append(sub_df_test[existing_metadatas + selected_columns])\n",
    "                        time_taken = round(time_taken, 5)\n",
    "                        save_file_name = os.path.join(two_hop_out_dir, \"{}_sec{}.pickle\".format(two_hop_desc, time_taken))\n",
    "                        to_save = pd.concat(processed, axis=0, ignore_index=True)\n",
    "                        to_save_test = pd.concat(processed_test, axis=0, ignore_index=True)\n",
    "                        with open(save_file_name, \"wb\") as fout:\n",
    "                            pickle.dump((to_save, to_save_test), fout)\n",
    "    \n",
    "    if prev_algo != 'RowSampling':\n",
    "        for keepFrac in reduction_strengths:\n",
    "            # SMF FBS ROW sampling for all cases\n",
    "            if prev_algo == 'RowAgg':\n",
    "                str_desc = get_str_desc_of_reduction_function('RowSampling', granularity, method='smf', dir='row', model='fbs', keepFrac=keepFrac)\n",
    "                two_hop_desc = one_hop_str + \"&\" + str_desc\n",
    "                print(two_hop_desc)\n",
    "                if not if_file_w_prefix_exists(two_hop_out_dir, two_hop_desc):\n",
    "                    processed, processed_test = list(), list()\n",
    "                    time_taken = 0\n",
    "                    for key, sub_df in grb_gran:\n",
    "                        start_time = time.time()\n",
    "                        selected_idx = sampling_based_reduction(\n",
    "                            sub_df[existing_metrics].values,\n",
    "                            None,\n",
    "                            method='smf',\n",
    "                            dir='row',\n",
    "                            model='fbs',\n",
    "                            keepFrac=keepFrac\n",
    "                        )\n",
    "                        end_time = time.time()\n",
    "                        time_taken += end_time - start_time\n",
    "                        sub_df_test = safe_get_subgroup(grb_gran_test, key)\n",
    "                        processed.append(sub_df.iloc[selected_idx])\n",
    "                        if sub_df_test is not None:\n",
    "                            processed_test.append(sub_df_test)\n",
    "                    time_taken = round(time_taken, 5)\n",
    "                    save_file_name = os.path.join(two_hop_out_dir, \"{}_sec{}.pickle\".format(two_hop_desc, time_taken))\n",
    "                    to_save = pd.concat(processed, axis=0, ignore_index=True)\n",
    "                    to_save_test = pd.concat(processed_test, axis=0, ignore_index=True)\n",
    "                    with open(save_file_name, \"wb\") as fout:\n",
    "                        pickle.dump((to_save, to_save_test), fout)\n",
    "\n",
    "            # SMF FLS, SSP only if aggregation is previously applied\n",
    "            if prev_algo == 'RowAgg':\n",
    "                for method in [\"smf\", \"ssp\"]:\n",
    "                    if method == \"smf\":\n",
    "                        str_desc = get_str_desc_of_reduction_function( \"RowSampling\", granularity, method=method, dir='row', model='fls', keepFrac=keepFrac)\n",
    "                        two_hop_desc = one_hop_str + \"&\" + str_desc\n",
    "                        if not if_file_w_prefix_exists(two_hop_out_dir, two_hop_desc):\n",
    "                            processed = list()\n",
    "                            processed_test = list()\n",
    "                            time_taken = 0\n",
    "                            for key, sub_df in grb_gran:\n",
    "                                start_time = time.time()\n",
    "                                selected_idx = sampling_based_reduction(\n",
    "                                    sub_df[existing_metrics].values,\n",
    "                                    None,\n",
    "                                    method=method,\n",
    "                                    dir='row',\n",
    "                                    model='fls',\n",
    "                                    keepFrac=keepFrac\n",
    "                                )\n",
    "                                end_time = time.time()\n",
    "                                time_taken += end_time - start_time\n",
    "                                \n",
    "                                sub_df_test = safe_get_subgroup(grb_gran_test, key)\n",
    "                                processed.append(sub_df.iloc[selected_idx])\n",
    "                                if sub_df_test is not None:\n",
    "                                    processed_test.append(sub_df_test)  # no need to row-sampling on test data\n",
    "\n",
    "                            time_taken = round(time_taken, 5)\n",
    "                            save_file_name = os.path.join(two_hop_out_dir, \"{}_sec{}.pickle\".format(two_hop_desc, time_taken))\n",
    "\n",
    "                            to_save = pd.concat(processed, axis=0, ignore_index=True)\n",
    "                            to_save_test = pd.concat(processed_test, axis=0, ignore_index=True)\n",
    "                            \n",
    "                            with open(save_file_name, \"wb\") as fout:\n",
    "                                pickle.dump((to_save, to_save_test), fout)\n",
    "\n",
    "                    elif method == \"ssp\":\n",
    "\n",
    "                        for sampler in ['doublePhase', 'leverage']:\n",
    "                            str_desc = get_str_desc_of_reduction_function('RowSampling', granularity, method='ssp', dir='row', sampler=sampler, keepFrac=keepFrac)\n",
    "                            two_hop_desc = one_hop_str + \"&\" + str_desc\n",
    "\n",
    "                            if not if_file_w_prefix_exists(two_hop_out_dir, two_hop_desc):\n",
    "                                processed = list()\n",
    "                                processed_test = list()\n",
    "                                time_taken = 0\n",
    "                                for key, sub_df in grb_gran:\n",
    "                                    start_time = time.time()\n",
    "                                    selected_idx = sampling_based_reduction(\n",
    "                                        sub_df[existing_metrics].values,\n",
    "                                        None,\n",
    "                                        method='ssp',\n",
    "                                        dir='row',\n",
    "                                        sampler=sampler,\n",
    "                                        keepFrac=keepFrac\n",
    "                                    )\n",
    "                                    end_time = time.time()\n",
    "                                    time_taken += end_time - start_time\n",
    "                                    \n",
    "                                    sub_df_test = safe_get_subgroup(grb_gran_test, key)\n",
    "                                    processed.append(sub_df.iloc[selected_idx])\n",
    "                                    if sub_df_test is not None:\n",
    "                                        processed_test.append(sub_df_test)\n",
    "\n",
    "                                time_taken = int(time_taken)\n",
    "                                save_file_name = os.path.join(\n",
    "                                    two_hop_out_dir, \"{}_sec{}.pickle\".format(two_hop_desc, time_taken)\n",
    "                                )\n",
    "\n",
    "                                to_save = pd.concat(processed, axis=0, ignore_index=True)\n",
    "                                to_save_test = pd.concat(processed_test, axis=0, ignore_index=True)\n",
    "                                \n",
    "                                with open(save_file_name, \"wb\") as fout:\n",
    "                                    pickle.dump((to_save, to_save_test), fout)\n",
    "    \n",
    "    \n",
    "    if prev_algo != 'RowAgg':\n",
    "        agg_cols = [x for x in train_df.columns if x not in scout_metadata]\n",
    "        agg_cols = ['IncidentId', ] + agg_cols\n",
    "\n",
    "        # for option in [1, 2, 3, ]:\n",
    "        for option in [1, ]:\n",
    "            str_desc = get_str_desc_of_reduction_function(\"RowAgg\", granularity, dir=\"row\", grb=\"IncidentId\", option=option)\n",
    "            two_hop_desc = one_hop_str + \"&\" + str_desc\n",
    "            if not if_file_w_prefix_exists(two_hop_out_dir, two_hop_desc):\n",
    "                processed = list()\n",
    "                processed_test = list()\n",
    "                time_taken = 0\n",
    "                for keys, sub_df in grb_gran:\n",
    "                    start_time = time.time()\n",
    "                    aggregated_result = aggregation_based_reduction(sub_df[agg_cols], dir=\"row\", grb='IncidentId', option=option)\n",
    "                    time_taken += time.time() - start_time\n",
    "                    rename_col = list()\n",
    "                    for old_col in aggregated_result.columns:\n",
    "                        rename_col.append(\":\".join(old_col))\n",
    "                    aggregated_result.columns = rename_col\n",
    "                    aggregated_result.reset_index(inplace=True)\n",
    "                    if type(granularity) != str:\n",
    "                        for gran_name, gran_val in zip(granularity, keys):\n",
    "                            aggregated_result[gran_name] = gran_val\n",
    "                    else:\n",
    "                        aggregated_result[granularity] = keys\n",
    "                    processed.append(aggregated_result)\n",
    "                    sub_df_test = safe_get_subgroup(grb_gran_test, keys)\n",
    "                    if sub_df_test is not None:\n",
    "                        aggregated_result_test = aggregation_based_reduction(sub_df_test[agg_cols], dir=\"row\", grb='IncidentId', option=option)\n",
    "                        rename_col_test = list()\n",
    "                        for old_col in aggregated_result_test.columns:\n",
    "                            rename_col_test.append(\":\".join(old_col))\n",
    "                        aggregated_result_test.columns = rename_col\n",
    "                        aggregated_result_test.reset_index(inplace=True)\n",
    "\n",
    "                        if type(granularity) != str:\n",
    "                            for gran_name, gran_val in zip(granularity, keys):\n",
    "                                aggregated_result_test[gran_name] = gran_val\n",
    "                        else:\n",
    "                            aggregated_result_test[granularity] = keys\n",
    "                        \n",
    "                        processed_test.append(aggregated_result_test)\n",
    "\n",
    "                to_save = pd.concat(processed, axis=0, ignore_index=True)\n",
    "                to_save_test = pd.concat(processed_test, axis=0, ignore_index=True)\n",
    "\n",
    "                # reorganize columns\n",
    "                metadata_left = [x for x in scout_metadata if x in granularity]\n",
    "                metrics_left = [x for x in to_save.columns if x not in scout_metadata]\n",
    "                out_columns = ['IncidentId', ] + metadata_left + metrics_left\n",
    "                time_taken = round(time_taken, 5)\n",
    "                save_file_name = os.path.join(two_hop_out_dir, \"{}_sec{}.pickle\".format(two_hop_desc, time_taken))\n",
    "                to_save, to_save_test = to_save[out_columns], to_save_test[out_columns]\n",
    "                with open(save_file_name, \"wb\") as fout:\n",
    "                    pickle.dump((to_save, to_save_test), fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### EVALUATE SCOUT GUIDED TWO-HOP REDUCTIONS #####\n",
    "if dummy:\n",
    "    guided_two_hop_save_dir = os.path.join(scout_data_dir, scout_guided_reduction_save_dir, \"two_hop\")\n",
    "    eval_dir = scout_dummy_automl_eval_dir\n",
    "else:\n",
    "    guided_two_hop_save_dir = os.path.join(scout_azure_dbfs_dir, scout_guided_reduction_save_dir, \"two_hop\")\n",
    "    eval_dir = scout_dbfs_automl_eval_dir\n",
    "os.system(\"mkdir -p {}\".format(eval_dir))\n",
    "\n",
    "reduced_instances = [x for x in os.listdir(guided_two_hop_save_dir) if x.endswith(\".pickle\")]\n",
    "for red in reduced_instances:\n",
    "    out_path = os.path.join(eval_dir, red.replace(\"pickle\", \"json\"))\n",
    "    print(out_path)\n",
    "    if not os.path.isfile(out_path):\n",
    "        scout_evaluator(os.path.join(guided_two_hop_save_dir, red), out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SAME DATASET, DIFFERENT PURPOSE #####\n",
    "# evaluate scout naive reduction for severity prediction\n",
    "if dummy:\n",
    "    naive_save_dir = os.path.join(scout_data_dir, scout_naive_reduction_save_dir, \"one_hop\")\n",
    "    eval_dir = os.path.join(scout_dummy_automl_eval_dir, 'one_hop_sp')\n",
    "else:\n",
    "    naive_save_dir = os.path.join(scout_azure_dbfs_dir, scout_naive_reduction_save_dir, \"one_hop\")\n",
    "    eval_dir = os.path.join(scout_dbfs_automl_eval_dir, 'one_hop_sp')\n",
    "os.system(\"mkdir -p {}\".format(eval_dir))\n",
    "\n",
    "reduced_instances = [x for x in os.listdir(naive_save_dir) if x.endswith(\".pickle\")]\n",
    "for red in reduced_instances:\n",
    "    out_path = os.path.join(eval_dir, red.replace(\"pickle\", \"json\"))\n",
    "    print(out_path)\n",
    "    if not os.path.isfile(out_path):\n",
    "        scout_evaluator(os.path.join(naive_save_dir, red), out_path, objective='severityPrediction')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('redeng')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3cb91839837c12cfcee7277f5c3320ca38bd1b0df530efe2c625284ea85332e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
